{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Training a Reward Model on Mac M3 (Hugging Face)\n",
    "\n",
    "Yes, you can run Hugging Face training on your Apple M3!\n",
    "We use **PyTorch MPS (Metal Performance Shaders)** to accelerate training on your Mac's GPU.\n",
    "\n",
    "**Goal**: Train a small \"Reward Model\" that can judge if an answer is correct.\n",
    "This replaces the \"Rule-Based Verifier\" from Lesson 3 with a learned neural network.\n",
    "\n",
    "### Prerequisites\n",
    "*   `pip install trl peft`\n",
    "*   **Note**: Training requires significant RAM. We will use a tiny model (`Qwen/Qwen2.5-0.5B-Instruct`) to ensure it fits comfortably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 1. Setup MPS (Apple Silicon)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data\n",
    "To train a Reward Model, we need **Pairs** of (Chosen, Rejected) responses.\n",
    "We will load a tiny slice of the `anthropic/hh-rlhf` dataset (or a math preference set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct a tiny synthetic dataset for demonstration\n",
    "# Format: {prompt, chosen, rejected}\n",
    "data = [\n",
    "    {\"prompt\": \"2+2=\", \"chosen\": \"The answer is 4.\", \"rejected\": \"The answer is 5.\"},\n",
    "    {\"prompt\": \"Is 91 prime?\", \"chosen\": \"No, 91 is 7*13.\", \"rejected\": \"Yes, 91 is prime.\"},\n",
    "    {\"prompt\": \"Capital of France?\", \"chosen\": \"Paris\", \"rejected\": \"London\"}\n",
    "] * 10 # Repeat to fake a batch\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "print(\"Dataset prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model (with LoRA)\n",
    "We use **LoRA (Low-Rank Adaptation)**. Instead of training the whole model (heavy), we train small adapter layers (light).\n",
    "We use `Qwen/Qwen2.5-0.5B-Instruct` because it is small and fast on Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model with Classification Head (1 label = Score)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=1, \n",
    "    torch_dtype=torch.float16, # Half precision for M3 speed\n",
    "    device_map=device\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define LoRA Config\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "# NOTE: We do NOT define get_peft_model(model) here because RewardTrainer does it for us\n",
    "print(\"Model and PEFT Config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train on M3\n",
    "TRL requires `RewardConfig` (not just vanilla `TrainingArguments`) for TRL v0.8+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RewardConfig instead of TrainingArguments\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model_output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=1,\n",
    "    use_mps_device=True, # IMPORTANT: Tells HF to use M3 Metal\n",
    "    bf16=False, \n",
    "    fp16=False, # FIXED: Disabled to avoid MPS unscale error on simple setup\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"Starting Training on M3...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference\n",
    "Now we can use this trained model to score new answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answer(prompt, answer):\n",
    "    inputs = tokenizer(prompt, answer, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        score = model(**inputs).logits[0].cpu().item()\n",
    "    return score\n",
    "\n",
    "s1 = score_answer(\"Is 91 prime?\", \"No, it is 7*13.\")\n",
    "s2 = score_answer(\"Is 91 prime?\", \"Yes it is.\")\n",
    "\n",
    "print(f\"Score (Correct): {s1:.4f}\")\n",
    "print(f\"Score (Wrong): {s2:.4f}\")\n",
    "if s1 > s2:\n",
    "    print(\"SUCCESS: Model prefers the correct answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification: Did it actually learn?\n",
    "We can check the training logs to see if the **Loss** decreased.\n",
    "Loss going down means the model got better at distinguishing Correct vs Wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.state.log_history\n",
    "if history:\n",
    "    initial_loss = history[0].get('loss', 'N/A')\n",
    "    final_loss = history[-1].get('loss', 'N/A') if history else 'N/A'\n",
    "    print(f\"Initial Loss: {initial_loss}\")\n",
    "    print(f\"Final Loss:   {final_loss}\")\n",
    "    print(\"\\n(If Final < Initial, the model effectively learned!)\")\n",
    "else:\n",
    "    print(\"Loss history not found (run training first).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
