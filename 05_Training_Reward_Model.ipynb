{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Training a Reward Model on Mac M3 (Hugging Face)\n",
    "\n",
    "## ðŸŽ TTRL on Apple Silicon\n",
    "So far, we have used \"Rule-Based Verifiers\" (asking the LLM \"Is this correct?\").\n",
    "But in production systems like ChatGPT or DeepSeek, we use **Trained Reward Models**.\n",
    "\n",
    "A Reward Model is a specialized neural network trained to output a score representing quality. It is faster and more accurate than asking an LLM to chat about itself.\n",
    "\n",
    "**In this lesson:**\n",
    "1.  We set up **PyTorch MPS** (Metal Performance Shaders) to train on your Mac M3.\n",
    "2.  We use Hugging Face `TRL` (Transformer Reinforcement Learning).\n",
    "3.  We train a **LoRA (Low-Rank Adapter)** to classify \"Good Math\" vs \"Bad Math\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 1. Setup MPS (Apple Silicon)\n",
    "# This checks if your Mac's GPU is available for math acceleration.\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Step 1: The Preference Dataset\n",
    "Reward models learn from **Comparisons**. They need to know what is \"Better\".\n",
    "\n",
    "Structure: `{Prompt, Chosen (Winner), Rejected (Loser)}`\n",
    "\n",
    "We create a tiny synthetic dataset here to demonstrate the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct a tiny synthetic dataset for demonstration\n",
    "data = [\n",
    "    {\"prompt\": \"2+2=\", \"chosen\": \"The answer is 4.\", \"rejected\": \"The answer is 5.\"},\n",
    "    {\"prompt\": \"Is 91 prime?\", \"chosen\": \"No, 91 is 7*13.\", \"rejected\": \"Yes, 91 is prime.\"},\n",
    "    {\"prompt\": \"Capital of France?\", \"chosen\": \"Paris\", \"rejected\": \"London\"}\n",
    "] * 10 # Repeat 10 times so the trainer has enough batches to run\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "print(\"Dataset prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Step 2: The Model with LoRA\n",
    "We load `Qwen2.5-0.5B`. This is a very small, fast model perfect for Mac experimentation.\n",
    "\n",
    "**What is specific here?**\n",
    "*   `AutoModelForSequenceClassification`: We aren't generating text. We are outputting a single number (Score).\n",
    "*   `num_labels=1`: The output is a scaler value (Reward).\n",
    "*   `LoraConfig`: We freeze the gigabytes of main weights, and only train a tiny 10MB adapter. This makes training feasible on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model with Classification Head (1 label = Score)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=1, \n",
    "    torch_dtype=torch.float16, # Half precision for M3 speed\n",
    "    device_map=device\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define LoRA Config\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "print(\"Model and PEFT Config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Step 3: Training with TRL\n",
    "We use the `RewardTrainer`. It takes our pairs and uses a **Ranking Loss**.\n",
    "It adjusts the weights so that `Score(Chosen) > Score(Rejected)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RewardConfig instead of TrainingArguments\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model_output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=1,\n",
    "    use_mps_device=True, # IMPORTANT: Tells HF to use M3 Metal\n",
    "    bf16=False, \n",
    "    fp16=False, # Stabilize MPS\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"Starting Training on M3...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª Step 4: Verification (Inference)\n",
    "We verify the model learned something.\n",
    "We pass in the \"Wrong\" answer and the \"Right\" answer for the Prime problem.\n",
    "If the score for \"Right\" is higher, the training worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answer(prompt, answer):\n",
    "    inputs = tokenizer(prompt, answer, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        score = model(**inputs).logits[0].cpu().item()\n",
    "    return score\n",
    "\n",
    "s1 = score_answer(\"Is 91 prime?\", \"No, it is 7*13.\")\n",
    "s2 = score_answer(\"Is 91 prime?\", \"Yes it is.\")\n",
    "\n",
    "print(f\"Score (Correct): {s1:.4f}\")\n",
    "print(f\"Score (Wrong): {s2:.4f}\")\n",
    "if s1 > s2:\n",
    "    print(\"SUCCESS: Model prefers the correct answer.\")\n",
    "else:\n",
    "    print(\"FAIL: Model still prefers the wrong answer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
