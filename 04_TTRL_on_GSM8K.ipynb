{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Scaling TTRL on GSM8K\n",
    "\n",
    "## üèãÔ∏è Benchmarking the Experiment\n",
    "We've seen TTRL work on single examples. But is it actually reliable? \n",
    "To solve this, we need a **Benchmark**.\n",
    "\n",
    "**The Dataset: GSM8K (Grade School Math 8K)**\n",
    "This is the gold-standard dataset for LLM logical reasoning. It contains 8,500 high-quality math word problems created by humans.\n",
    "\n",
    "**Our Experiment**:\n",
    "We will run a head-to-head comparison:\n",
    "1.  **Baseline**: Standard Mistral (One guess, Temperature 0).\n",
    "2.  **TTRL Agent**: Mistral + Best-of-3 Search + Self-Verification.\n",
    "\n",
    "We expect the TTRL Agent to achieve a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "try:\n",
    "    import ollama\n",
    "except ImportError:\n",
    "    print(\"pip install ollama\")\n",
    "\n",
    "console = Console()\n",
    "MODEL_NAME = \"mistral:7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Step 1: Loading the Data\n",
    "We use the Hugging Face `datasets` library to pull GSM8K.\n",
    "The dataset has a `test` split which is what we use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"[yellow]Loading GSM8K...[/yellow]\")\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "# For this tutorial, we select 5 examples for speed.\n",
    "# In a real eval, you would use 100+.\n",
    "examples = dataset.select(range(5))\n",
    "\n",
    "def extract_answer(text: str):\n",
    "    \"\"\"Extracts the number after #### in GSM8K solutions\"\"\"\n",
    "    match = re.search(r'####\\s*(\\d+)', text)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Step 2: The Evaluation Engine\n",
    "We define a function `solve_problem` that can toggle between modes.\n",
    "\n",
    "*   **`method='greedy'`**: What most people use. Fast, cheap, often wrong.\n",
    "*   **`method='ttrl'`**: Our agentic loop. Slower, more expensive, but smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(prompt: str, method=\"greedy\") -> str:\n",
    "    \"\"\"Solves using either Greedy (Baseline) or TTRL (Best-of-N)\"\"\"\n",
    "    \n",
    "    if method == \"greedy\":\n",
    "        # Simple Zero-Shot\n",
    "        response = ollama.chat(model=MODEL_NAME, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response['message']['content']\n",
    "    \n",
    "    elif method == \"ttrl\":\n",
    "        # Best-of-3 with Verification\n",
    "        best_score = -1\n",
    "        best_ans = \"\"\n",
    "        \n",
    "        for _ in range(3):\n",
    "            # 1. Generate Proposal (High Temp)\n",
    "            cand = ollama.chat(model=MODEL_NAME, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0.8})\n",
    "            content = cand['message']['content']\n",
    "            \n",
    "            # 2. Verify Proposal (Self-Check)\n",
    "            check_prompt = f\"Question: {prompt}\\nAnswer: {content}\\nIs this correct? Reply 1 for Yes, 0 for No.\"\n",
    "            check = ollama.chat(model=MODEL_NAME, messages=[{\"role\": \"user\", \"content\": check_prompt}], options={\"temperature\":0})\n",
    "            \n",
    "            score = 1.0 if \"1\" in check['message']['content'] else 0.0\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_ans = content\n",
    "        \n",
    "        return best_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Step 3: Running the Experiment\n",
    "We iterate through the problems and record wins/losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table(title=\"GSM8K Results\")\n",
    "table.add_column(\"Problem\", style=\"dim\", width=30)\n",
    "table.add_column(\"Baseline\", justify=\"center\")\n",
    "table.add_column(\"TTRL\", justify=\"center\")\n",
    "table.add_column(\"Truth\", justify=\"center\")\n",
    "\n",
    "for ex in examples:\n",
    "    q = ex['question']\n",
    "    truth = extract_answer(ex['answer'])\n",
    "    \n",
    "    # Run Baseline\n",
    "    base_raw = solve_problem(q, method=\"greedy\")\n",
    "    base_correct = truth in base_raw # Simple string match for tutorial\n",
    "    \n",
    "    # Run TTRL\n",
    "    ttrl_raw = solve_problem(q, method=\"ttrl\")\n",
    "    ttrl_correct = truth in ttrl_raw\n",
    "    \n",
    "    table.add_row(\n",
    "        q[:30]+\"...\", \n",
    "        \"‚úÖ\" if base_correct else \"‚ùå\", \n",
    "        \"‚úÖ\" if ttrl_correct else \"‚ùå\", \n",
    "        truth\n",
    "    )\n",
    "\n",
    "console.print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
