{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: End-to-End Adaptation with Real Models\n",
    "\n",
    "## 1. Overview\n",
    "Now we move from toy simulations to **Real World TTT** using standard libraries (`transformers`, `peft`).\n",
    "\n",
    "### The Objective\n",
    "To test if a model can learn a **Secret Password** that appears in the context, using *only* gradient updates (no prompt injection).\n",
    "- This proves the model isn't just \"copying\" from its input buffer (like in standard Attention).\n",
    "- It proves the model has **stored the information in its weights**.\n",
    "\n",
    "### The Technology Stack\n",
    "1.  **GPT-2**: Our base Language Model. It knows English, but it doesn't know our secret password.\n",
    "2.  **LoRA (Low-Rank Adaptation)**: \n",
    "    - Instead of updating the massive GPT-2 model (slow, heavy), we attach tiny adapter matrices.\n",
    "    - We train *only* these adapters on the context.\n",
    "    - This makes TTT standard-hardware friendly (even on laptops!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Loading the Model\n",
    "\n",
    "**Why Freeze?**\n",
    "We freeze the base model parameters because we treat GPT-2's knowledge as \"Long-Term Memory\" (General Intelligence). We don't want to corrupt it with temporary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2 on mps...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available(): device = \"cuda\"\n",
    "\n",
    "print(f\"Loading {model_id} on {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# FREEZE MATRIX: We stop the base model from updating.\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Adaptation Engine\n",
    "\n",
    "This function contains the core **Inner Loop** logic. It represents the \"Training\" phase of TTT.\n",
    "\n",
    "### Logic Flow\n",
    "1.  **Inject LoRA**: Add new, trainable weights to the model. These act as our \"Short-Term Memory\".\n",
    "2.  **Training Loop**: \n",
    "    - Forward Pass: Read the `text_chunk`.\n",
    "    - Backward Pass: Calculate how wrong the model was at predicting this chunk.\n",
    "    - Update: Adjust the LoRA weights to minimize this error.\n",
    "3.  **Return**: The adapted model, now containing the context information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_lora_adapt(base_model, text_chunk, learning_rate=1e-3, num_steps=10):\n",
    "    \"\"\"\n",
    "    Performs Test-Time Training via LoRA on the given text chunk.\n",
    "    \"\"\"\n",
    "    # A. Define Adapter Config\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        r=8,              # Rank: Low rank means fewer parameters to update\n",
    "        lora_alpha=32,    # Alpha: Scaling factor for updates\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # B. Attach Adapter\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    model.print_trainable_parameters() # Verification: Should be small %\n",
    "    \n",
    "    # C. Setup Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Prepare Inputs\n",
    "    inputs = tokenizer(text_chunk, return_tensors=\"pt\").to(device)\n",
    "    labels = inputs.input_ids.clone()\n",
    "    \n",
    "    # D. The Training Loop (Inner Loop)\n",
    "    print(\"Starting Adaptation...\")\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass (Calculate Error)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward Pass (Calculate Gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 2 == 0:\n",
    "             print(f\"  Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Experiment: Learning a Secret\n",
    "\n",
    "We create a piece of information that **does not exist** in the public internet (and thus is not in GPT-2's training data).\n",
    "\n",
    "- **Secret**: \"The operational password for Project Omega is 'BlueSky99'.\"\n",
    "\n",
    "We will first verify that the model **doesn't** know this. Then we will adapt it and check if it learns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline (Zero-Shot) ---\n",
      "Baseline Output: The operational password for Project Omega is \"C:\\Program Files\\Omega\\O\n",
      "\n",
      "--- Performing Test-Time Training ---\n",
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n",
      "Starting Adaptation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 0: Loss = 0.8360\n",
      "  Step 2: Loss = 0.6877\n",
      "  Step 4: Loss = 0.5766\n",
      "  Step 6: Loss = 0.4883\n",
      "  Step 8: Loss = 0.3710\n",
      "  Step 10: Loss = 0.2896\n",
      "  Step 12: Loss = 0.1617\n",
      "  Step 14: Loss = 0.0993\n",
      "\n",
      "--- Adapted Generation ---\n",
      "Adapted Output: The operational password for Project Omega is 'BlueSky99'. The operational password for Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret_info = \"The operational password for Project Omega is 'BlueSky99'. \"\n",
    "# Repeating it to simulate a strong signal / longer document mention\n",
    "long_context_text = secret_info * 10 \n",
    "\n",
    "query = \"The operational password for Project Omega is\"\n",
    "\n",
    "# --- STEP 1: Baseline Check ---\n",
    "print(\"\\n--- Baseline (Zero-Shot) ---\")\n",
    "inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    # We expect this to fail (generate nonsense)\n",
    "    gen_tokens = base_model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"Baseline Output: {tokenizer.decode(gen_tokens[0])}\")\n",
    "\n",
    "# --- STEP 2: TTT (Adaptation) ---\n",
    "print(\"\\n--- Performing Test-Time Training ---\")\n",
    "# This step 'compresses' the text into the adapter weights\n",
    "adapted_model = ttt_lora_adapt(base_model, long_context_text, learning_rate=1e-3, num_steps=15)\n",
    "\n",
    "# --- STEP 3: Verification Check ---\n",
    "print(\"\\n--- Adapted Generation ---\")\n",
    "with torch.no_grad():\n",
    "    # We verify if the model can recall the secret from its weights\n",
    "    gen_tokens_adapted = adapted_model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"Adapted Output: {tokenizer.decode(gen_tokens_adapted[0])}\")\n",
    "\n",
    "# --- STEP 4: Cleanup ---\n",
    "# Good practice: un-attach the adapter so the base model is clean for the next test\n",
    "adapted_model.unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
