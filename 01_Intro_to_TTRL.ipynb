{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: The Intuition of TTRL (with Real Local LLM)\n",
    "\n",
    "**Test-Time Reinforcement Learning (TTRL)** is based on a simple premise:\n",
    "*If we let a model \"think\" longer by exploring multiple paths, it can verify and correct itself.*\n",
    "\n",
    "In this lesson, we perform **Generative Consensus** using your local **Ollama** model.\n",
    "\n",
    "We will:\n",
    "1.  Ask a logic question (`mistral:7b` often gets tricks wrong).\n",
    "2.  Generate diverse answers (Test-Time Scaling).\n",
    "3.  Use \"Majority Voting\" to define a **Pseudo-Reward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "try:\n",
    "    import ollama\n",
    "except ImportError:\n",
    "    print(\"Please run: pip install ollama\")\n",
    "\n",
    "console = Console()\n",
    "MODEL_NAME = \"mistral:7b\"  # Your local model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to the Real Model\n",
    "We define a helper function `get_answer` that queries your local Ollama instance.\n",
    "Note the `temperature` parameter: High temperature means more creativity/diversity, which is essential for TTRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Connecting to Local Model:</span> mistral:7b\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mConnecting to Local Model:\u001b[0m mistral:7b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(f\"[bold]Connecting to Local Model:[/bold] {MODEL_NAME}\")\n",
    "\n",
    "def get_answer(prompt: str, temp: float = 0.7) -> str:\n",
    "    \"\"\"Gets a single completion from Ollama.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a concise assistant. Answer with only the result.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            options={\"temperature\": temp}\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem: Conjunction Fallacy\n",
    "We use the classic \"Linda Problem\".\n",
    "\n",
    "*   **Option A**: Linda is a bank teller.\n",
    "*   **Option B**: Linda is a bank teller AND active in feminist movement.\n",
    "\n",
    "**Logic Rule**: Probability(A) >= Probability(A + B). \n",
    "Therefore, **A** is mathematically more likely, even if the description sounds like B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Problem:</span> Linda Problem <span style=\"font-weight: bold\">(</span>Conjunction Fallacy<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mProblem:\u001b[0m Linda Problem \u001b[1m(\u001b[0mConjunction Fallacy\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROBLEM = \"\"\"\n",
    "Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice.\n",
    "Which is more probable?\n",
    "A) Linda is a bank teller.\n",
    "B) Linda is a bank teller and is active in the feminist movement.\n",
    "Answer with only 'A' or 'B'.\n",
    "\"\"\"\n",
    "\n",
    "console.print(f\"\\n[bold cyan]Problem:[/bold cyan] Linda Problem (Conjunction Fallacy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test-Time Scaling (Sampling)\n",
    "Instead of asking once, we ask **5 times** with high temperature (0.9).\n",
    "This simulates \"thinking about it from different angles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Sampling...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mSampling\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Real Generated Samples</span>\n",
       "<span style=\"font-style: italic\">        (N=5)         </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Sample ID </span>┃<span style=\"font-weight: bold\"> Answer </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1         </span>│   <span style=\"color: #008000; text-decoration-color: #008000\">A</span>    │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2         </span>│   <span style=\"color: #008000; text-decoration-color: #008000\">A</span>    │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3         </span>│   <span style=\"color: #008000; text-decoration-color: #008000\">A</span>    │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4         </span>│   <span style=\"color: #008000; text-decoration-color: #008000\">A</span>    │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5         </span>│   <span style=\"color: #008000; text-decoration-color: #008000\">A</span>    │\n",
       "└───────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mReal Generated Samples\u001b[0m\n",
       "\u001b[3m        (N=5)         \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mSample ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAnswer\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m1        \u001b[0m\u001b[2m \u001b[0m│   \u001b[32mA\u001b[0m    │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2        \u001b[0m\u001b[2m \u001b[0m│   \u001b[32mA\u001b[0m    │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3        \u001b[0m\u001b[2m \u001b[0m│   \u001b[32mA\u001b[0m    │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4        \u001b[0m\u001b[2m \u001b[0m│   \u001b[32mA\u001b[0m    │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5        \u001b[0m\u001b[2m \u001b[0m│   \u001b[32mA\u001b[0m    │\n",
       "└───────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_SAMPLES = 5\n",
    "samples = []\n",
    "\n",
    "table = Table(title=f\"Real Generated Samples (N={N_SAMPLES})\")\n",
    "table.add_column(\"Sample ID\", style=\"dim\")\n",
    "table.add_column(\"Answer\", justify=\"center\")\n",
    "\n",
    "console.print(\"[yellow]Sampling...[/yellow]\")\n",
    "for i in range(N_SAMPLES):\n",
    "    # High temp for diversity\n",
    "    raw_ans = get_answer(PROBLEM, temp=0.9)\n",
    "    # Normalize answer to just A or B\n",
    "    ans = \"A\" if \"A\" in raw_ans and not \"B\" in raw_ans else \"B\"\n",
    "    if \"B\" in raw_ans: ans = \"B\"\n",
    "    \n",
    "    samples.append(ans)\n",
    "    color = \"green\" if ans == \"A\" else \"red\"\n",
    "    table.add_row(str(i+1), f\"[{color}]{ans}[/{color}]\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Majority Voting (Consensus)\n",
    "We count the votes. The most common answer becomes our **Consensus**.\n",
    "In TTRL, this Consensus acts as a **Pseudo-Reward** signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Consensus Analysis:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mConsensus Analysis:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Most Common Answer: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">A</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> votes<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Most Common Answer: \u001b[1;35mA\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m/\u001b[1;36m5\u001b[0m votes\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">SUCCESS:</span> The consensus matches the Ground Truth <span style=\"font-weight: bold\">(</span>A<span style=\"font-weight: bold\">)</span>!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ \u001b[1;32mSUCCESS:\u001b[0m The consensus matches the Ground Truth \u001b[1m(\u001b[0mA\u001b[1m)\u001b[0m!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = Counter(samples)\n",
    "consensus_answer, consensus_count = counts.most_common(1)[0]\n",
    "\n",
    "console.print(f\"\\n[bold]Consensus Analysis:[/bold]\")\n",
    "console.print(f\"Most Common Answer: [bold magenta]{consensus_answer}[/bold magenta] ({consensus_count}/{N_SAMPLES} votes)\")\n",
    "\n",
    "GROUND_TRUTH = \"A\"\n",
    "if consensus_answer == GROUND_TRUTH:\n",
    "    console.print(\"✅ [bold green]SUCCESS:[/bold green] The consensus matches the Ground Truth (A)!\")\n",
    "else:\n",
    "    console.print(\"❌ [bold red]FAILURE:[/bold red] The model fell for the fallacy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "You have just implemented the **Inference Loop** of TTRL.\n",
    "In a full training system, we would take this successful result (\"A\") and use it to update the model weights using PPO (Proximal Policy Optimization), rewarding the model for generating reasoning that leads to \"A\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
