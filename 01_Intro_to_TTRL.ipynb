{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: The Intuition of Test-Time Reinforcement Learning (TTRL)\n",
    "\n",
    "## üéì What is TTRL?\n",
    "Most LLMs are \"frozen\" after training. They generate an answer once, and if it's wrong, it stays wrong.\n",
    "\n",
    "**Test-Time Reinforcement Learning (TTRL)**, or *Inference-Time Compute*, changes this paradigm. Instead of trusting the first answer, we treat the model as a stochastic (random) generator. By letting it \"think\" multiple times and exploring different reasoning paths, we can statistically marginalize out errors.\n",
    "\n",
    "**In this lesson:**\n",
    "1.  We connect to your local **Mistral** model via Ollama.\n",
    "2.  We pose a problem famous for tricking human intuition (The **Linda Problem**).\n",
    "3.  We implement **Majority Voting**, the simplest form of TTRL, to fix the model's bias without any training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Step 1: Library Setup\n",
    "We use `ollama` as our interface to the local LLM. We also use `rich` to print beautiful tables in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:33.652278Z",
     "iopub.status.busy": "2026-01-19T00:58:33.652115Z",
     "iopub.status.idle": "2026-01-19T00:58:33.850198Z",
     "shell.execute_reply": "2026-01-19T00:58:33.849925Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Ensure Ollama is installed\n",
    "try:\n",
    "    import ollama\n",
    "except ImportError:\n",
    "    print(\"Please run: pip install ollama\")\n",
    "\n",
    "console = Console()\n",
    "MODEL_NAME = \"mistral:7b\"  # Ensure you have run `ollama pull mistral:7b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Step 2: The Model Interface\n",
    "We create a function `get_answer`.\n",
    "\n",
    "**Critical Concept: Temperature**\n",
    "*   **`temperature=0.0`**: The model is deterministic. It always gives the same answer. Good for code, bad for TTRL.\n",
    "*   **`temperature=0.9`**: The model takes risks. It generates diverse answers. **We need this** for TTRL to work, because we need a variety of opinions to vote on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:33.851746Z",
     "iopub.status.busy": "2026-01-19T00:58:33.851663Z",
     "iopub.status.idle": "2026-01-19T00:58:33.855678Z",
     "shell.execute_reply": "2026-01-19T00:58:33.855473Z"
    }
   },
   "outputs": [],
   "source": [
    "console.print(f\"[bold]Connecting to Local Model:[/bold] {MODEL_NAME}\")\n",
    "\n",
    "def get_answer(prompt: str, temp: float = 0.7) -> str:\n",
    "    \"\"\"Gets a single completion from Ollama.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a concise assistant. Answer with only the result.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            # We pass the temperature dynamically\n",
    "            options={\"temperature\": temp}\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Step 3: The \"Conjunction Fallacy\" Problem\n",
    "This is a classic cognitive science puzzle.\n",
    "\n",
    "*   **Option A**: Probability of event $X$.\n",
    "*   **Option B**: Probability of event $X$ AND event $Y$.\n",
    "\n",
    "Mathematically, $P(X) \\ge P(X \\cap Y)$. **A is always the correct answer.**\n",
    "However, LLMs (like humans) get distracted by the detailed description and often guess **B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:33.876444Z",
     "iopub.status.busy": "2026-01-19T00:58:33.876317Z",
     "iopub.status.idle": "2026-01-19T00:58:33.878751Z",
     "shell.execute_reply": "2026-01-19T00:58:33.878500Z"
    }
   },
   "outputs": [],
   "source": [
    "PROBLEM = \"\"\"\n",
    "Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice.\n",
    "Which is more probable?\n",
    "A) Linda is a bank teller.\n",
    "B) Linda is a bank teller and is active in the feminist movement.\n",
    "Answer with only 'A' or 'B'.\n",
    "\"\"\"\n",
    "\n",
    "console.print(f\"\\n[bold cyan]Problem:[/bold cyan] Linda Problem (Conjunction Fallacy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé≤ Step 4: Test-Time Scaling (Sampling)\n",
    "This is the core of TTRL: **Don't ask once. Ask many times.**\n",
    "\n",
    "We sample $N=5$ times with high temperature (`0.9`).\n",
    "*   Some samples might be \"lazy\" (System 1 bias).\n",
    "*   Some samples might be \"lucky\" or \"reasoned\" (System 2 correctness).\n",
    "\n",
    "We collect them all into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:33.880089Z",
     "iopub.status.busy": "2026-01-19T00:58:33.879992Z",
     "iopub.status.idle": "2026-01-19T00:58:39.130684Z",
     "shell.execute_reply": "2026-01-19T00:58:39.130204Z"
    }
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 5\n",
    "samples = []\n",
    "\n",
    "table = Table(title=f\"Real Generated Samples (N={N_SAMPLES})\")\n",
    "table.add_column(\"Sample ID\", style=\"dim\")\n",
    "table.add_column(\"Answer\", justify=\"center\")\n",
    "\n",
    "console.print(\"[yellow]Sampling...[/yellow]\")\n",
    "for i in range(N_SAMPLES):\n",
    "    # 1. GENERATE with High Temperature\n",
    "    raw_ans = get_answer(PROBLEM, temp=0.9)\n",
    "    \n",
    "    # 2. NORMALIZE (Data Cleaning)\n",
    "    # Models are chatty, we need to extract just 'A' or 'B'\n",
    "    ans = \"A\" if \"A\" in raw_ans and not \"B\" in raw_ans else \"B\"\n",
    "    if \"B\" in raw_ans: ans = \"B\"\n",
    "    \n",
    "    samples.append(ans)\n",
    "    \n",
    "    # Visualization\n",
    "    color = \"green\" if ans == \"A\" else \"red\"\n",
    "    table.add_row(str(i+1), f\"[{color}]{ans}[/{color}]\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó≥Ô∏è Step 5: Majority Voting (Pseudo-Reward)\n",
    "Now we aggregate the intelligence.\n",
    "\n",
    "We assume that **Truth is more robust than Error**. If the model hallucinates, it might hallucinate different things each time. But if it finds the logic, the logic is consistent.\n",
    "\n",
    "Therefore, the **Consensus** is likely the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:39.133995Z",
     "iopub.status.busy": "2026-01-19T00:58:39.133871Z",
     "iopub.status.idle": "2026-01-19T00:58:39.138647Z",
     "shell.execute_reply": "2026-01-19T00:58:39.138389Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = Counter(samples)\n",
    "consensus_answer, consensus_count = counts.most_common(1)[0]\n",
    "\n",
    "console.print(f\"\\n[bold]Consensus Analysis:[/bold]\")\n",
    "console.print(f\"Most Common Answer: [bold magenta]{consensus_answer}[/bold magenta] ({consensus_count}/{N_SAMPLES} votes)\")\n",
    "\n",
    "GROUND_TRUTH = \"A\"\n",
    "if consensus_answer == GROUND_TRUTH:\n",
    "    console.print(\"‚úÖ [bold green]SUCCESS:[/bold green] The consensus matches the Ground Truth (A)!\")\n",
    "    console.print(\"The swarm of agents outperformed the individual bias.\")\n",
    "else:\n",
    "    console.print(\"‚ùå [bold red]FAILURE:[/bold red] The model fell for the fallacy consistently.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
