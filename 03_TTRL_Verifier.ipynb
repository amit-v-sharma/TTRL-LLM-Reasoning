{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: The Verifier (System 2 Thinking)\n",
    "\n",
    "## üïµÔ∏è Fixing the \"Confidently Wrong\" Model\n",
    "In Lesson 2, `mistral:7b` confidently said 91 is prime. It failed because it was answering based on **intuition** (System 1).\n",
    "\n",
    "To fix this, we need **System 2** (Deliberative Reasoning). We need to force the model to \"Show Its Work\".\n",
    "\n",
    "**The Strategy: \"Best-of-N\" with Verification**\n",
    "1.  **Generator**: We ask the model not just for an answer, but for a **step-by-step proof**.\n",
    "2.  **Verifier**: We use a second LLM call (or the same LLM) to act as a \"Judge\" or \"Teacher\". It looks at the proof and checks for errors.\n",
    "3.  **Selector**: Instead of picking the most *common* answer (Voting), we pick the *highest scored* answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:59.588705Z",
     "iopub.status.busy": "2026-01-19T00:58:59.588586Z",
     "iopub.status.idle": "2026-01-19T00:58:59.767858Z",
     "shell.execute_reply": "2026-01-19T00:58:59.767526Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from rich.console import Console\n",
    "try:\n",
    "    import ollama\n",
    "except ImportError:\n",
    "    print(\"pip install ollama\")\n",
    "\n",
    "console = Console()\n",
    "MODEL_NAME = \"mistral:7b\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Step 1: The Generator (Thinker)\n",
    "We change our prompt. We don't say \"Answer Yes/No\".\n",
    "We say: **\"Check for factors. Show your work.\"**\n",
    "\n",
    "This triggers the model to output a **Chain of Thought (CoT)**. Often, the mere act of writing validation logic allows the model to catch its own error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:59.769553Z",
     "iopub.status.busy": "2026-01-19T00:58:59.769463Z",
     "iopub.status.idle": "2026-01-19T00:58:59.771451Z",
     "shell.execute_reply": "2026-01-19T00:58:59.771164Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_thought(prompt: str, temp: float = 0.7) -> str:\n",
    "    \"\"\"Generates a step-by-step reasoning trace.\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a math expert. Think step-by-step to check for factors. show your work.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        options={\"temperature\": temp}\n",
    "    )\n",
    "    return response['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Step 2: The Verifier (LLM-as-a-Judge)\n",
    "This is the crucial new component. \n",
    "\n",
    "We take the `solution` generated above, feed it back into the model, and ask: **\"Is this logic correct?\"**\n",
    "\n",
    "This works because **Evaluation is easier than Generation**. It is easier to check a math proof than to write one from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:59.772853Z",
     "iopub.status.busy": "2026-01-19T00:58:59.772761Z",
     "iopub.status.idle": "2026-01-19T00:58:59.774743Z",
     "shell.execute_reply": "2026-01-19T00:58:59.774500Z"
    }
   },
   "outputs": [],
   "source": [
    "def verify_solution(problem: str, solution: str) -> float:\n",
    "    \"\"\"\n",
    "    Asks the model to critique the reasoning. Returns a score 0.0 to 1.0.\n",
    "    \"\"\"\n",
    "    verifier_prompt = f\"\"\"\n",
    "    Problem: {problem}\n",
    "    Proposed Solution: {solution}\n",
    "    \n",
    "    Task: Check the math calculations in the solution. \n",
    "    If you find ANY calculation error (e.g. 7*13 != 91), score it 0.\n",
    "    If the reasoning is sound and concludes correctly, score it 1.\n",
    "    Reply with ONLY the score (0 or 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=MODEL_NAME, # Self-Correction\n",
    "        messages=[{\"role\": \"user\", \"content\": verifier_prompt}],\n",
    "        options={\"temperature\": 0.0} # Deterministic for judging\n",
    "    )\n",
    "    \n",
    "    content = response['message']['content'].strip()\n",
    "    return 1.0 if \"1\" in content else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé Step 3: Best-of-N Search Loop\n",
    "This is the algorithm used by **OpenAI o1** during inference time.\n",
    "\n",
    "1.  **Generate $N$** distinct thought processes.\n",
    "2.  **Score** each one.\n",
    "3.  **Filter**: Throw away the bad logic.\n",
    "\n",
    "Even if the model is wrong 80% of the time, we only need it to be right **once** to succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:58:59.775960Z",
     "iopub.status.busy": "2026-01-19T00:58:59.775875Z",
     "iopub.status.idle": "2026-01-19T00:59:09.284804Z",
     "shell.execute_reply": "2026-01-19T00:59:09.284443Z"
    }
   },
   "outputs": [],
   "source": [
    "PROBLEM = \"Is 91 a prime number?\"\n",
    "N_SAMPLES = 5\n",
    "\n",
    "console.print(f\"\\n[bold yellow]Running Best-of-N Search (N={N_SAMPLES})...[/bold yellow]\")\n",
    "console.print(f\"Problem: {PROBLEM}\")\n",
    "\n",
    "best_score = -1.0\n",
    "best_solution = \"\"\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    # 1. Generate\n",
    "    solution = generate_thought(PROBLEM, temp=0.8)\n",
    "    \n",
    "    # 2. Verify\n",
    "    score = verify_solution(PROBLEM, solution)\n",
    "    \n",
    "    # Logging\n",
    "    color = \"green\" if score > 0.5 else \"red\"\n",
    "    console.print(f\"\\n[bold]Sample {i+1} (Score: {score}):[/bold]\")\n",
    "    console.print(f\"[{color}]{solution[:150].replace('\\n', ' ')}...[/{color}]\")\n",
    "    \n",
    "    # 3. Selection\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_solution = solution\n",
    "        if score == 1.0:\n",
    "            console.print(\"[bold green]Found perfect solution! Stopping early.[/bold green]\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÜ Step 4: Final Result\n",
    "Did we find the truth?\n",
    "Usually, in these 5 samples, `mistral:7b` will produce at least one chain where it calculates `7 * 10 = 70`, `7 * 3 = 21`, `70 + 21 = 91`, and realizes 91 is divisible by 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T00:59:09.286227Z",
     "iopub.status.busy": "2026-01-19T00:59:09.286124Z",
     "iopub.status.idle": "2026-01-19T00:59:09.290573Z",
     "shell.execute_reply": "2026-01-19T00:59:09.290313Z"
    }
   },
   "outputs": [],
   "source": [
    "console.print(f\"\\n[bold cyan]üèÜ Best Verification Score:[/bold cyan] {best_score}\")\n",
    "\n",
    "if best_score == 1.0:\n",
    "    console.print(\"[bold green]TTRL SUCCESS:[/bold green] Found a valid reasoning path!\")\n",
    "    console.print(f\"Best Thought:\\n{best_solution}\")\n",
    "else:\n",
    "    console.print(\"[bold red]FAILURE:[/bold red] Could not find a verified solution.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
