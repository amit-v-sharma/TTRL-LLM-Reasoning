{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Foundational Concepts for Long Context\n",
    "\n",
    "## 1. Overview\n",
    "Before we can train models on long contexts, we must ensure they can *represent* long contexts mathematically.\n",
    "\n",
    "### The Challenges\n",
    "1.  **Positional Encoding**: How does the model know that token #100,000 comes after token #99,999? Standard techniques (like RoPE) often break down or \"alias\" at lengths they weren't trained on.\n",
    "2.  **Memory Management**: You cannot fit a 1M token sequence into GPU memory all at once for processing. \n",
    "\n",
    "### In this Notebook\n",
    "We explore **two key techniques** required for TTT:\n",
    "1.  **RoPE Scaling**: Adjusting the \"Theta\" parameter to handle millions of tokens without mathematical breakdown.\n",
    "2.  **Chunking**: The strategy of breaking infinite streams into manageable blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RoPE Scaling (The 'Theta' Parameter)\n",
    "\n",
    "### Why is this important?\n",
    "Rotary Embeddings work by rotating the vector representation of a token. The angle of rotation corresponds to its position.\n",
    "- **The Aliasing Problem**: If `Theta` (the base frequency) is small, the vectors rotate fast. For very long sequences, a token at position 100,000 might have rotated so many times that it looks identical to a token at position 50. This confuses the model.\n",
    "- **The Solution**: Increase `Theta`. This slows down the rotation, ensuring every position up to millions of tokens has a unique angle.\n",
    "\n",
    "Let's visualize the math behind this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Calculates the complex frequencies for RoPE.\n",
    "    \n",
    "    Args:\n",
    "        dim: Head dimension (size of the vector being rotated)\n",
    "        end: The maximum sequence length we expect to see\n",
    "        theta: The base frequency scaling factor (The 'Rotation Speed')\n",
    "    \"\"\"\n",
    "    # 1. Calculate frequencies: 1 / theta^(i / dim)\n",
    "    # This creates a spectrum of speeds: some dimensions rotate fast, others slow.\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # 2. Generate positions [0, 1, ..., end]\n",
    "    t = torch.arange(end, device=freqs.device) \n",
    "    \n",
    "    # 3. Outer product: combine positions with frequencies\n",
    "    # This gives us the specific angle for every position at every dimension.\n",
    "    freqs = torch.outer(t, freqs).float()  \n",
    "    \n",
    "    # 4. Convert to complex numbers (polar form) for efficient rotation\n",
    "    # e^(i * angle) = cos(angle) + i*sin(angle)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs) \n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Standard vs. Scaled Theta\n",
    "\n",
    "In the TTT paper, they recommend scaling `Theta` from the standard **10,000** (used in Llama 2) to **10,000,000** (10 Million).\n",
    "\n",
    "Let's see how this affects our ability to generate positions for a 128k token document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Standard Context (8192 tokens)\n",
      "   Frequency Table Shape: torch.Size([8192, 32])\n",
      "   Theta: 10,000 (Fast Rotation)\n",
      "\n",
      "2. Long Context (128000 tokens)\n",
      "   Frequency Table Shape: torch.Size([128000, 32])\n",
      "   Theta: 10,000,000 (Slow Rotation)\n",
      "   Note: This larger theta keeps positions unique over long distances.\n"
     ]
    }
   ],
   "source": [
    "dim = 64\n",
    "\n",
    "# Scenario A: Standard Model (e.g., Llama 2 Base)\n",
    "# Designed for 4k-8k context.\n",
    "seq_len_short = 8192\n",
    "theta_short = 10000.0 \n",
    "\n",
    "print(f\"\\n1. Standard Context ({seq_len_short} tokens)\")\n",
    "freqs_short = precompute_freqs_cis(dim, seq_len_short, theta_short)\n",
    "print(f\"   Frequency Table Shape: {freqs_short.shape}\")\n",
    "print(\"   Theta: 10,000 (Fast Rotation)\")\n",
    "\n",
    "\n",
    "# Scenario B: TTT / Long Context\n",
    "# Designed for 128k - 1M context.\n",
    "# As per TTT paper, we scale theta to 10 Million to prevent aliasing.\n",
    "seq_len_long = 128_000\n",
    "theta_long = 10_000_000.0\n",
    "\n",
    "print(f\"\\n2. Long Context ({seq_len_long} tokens)\")\n",
    "freqs_long = precompute_freqs_cis(dim, seq_len_long, theta_long)\n",
    "print(f\"   Frequency Table Shape: {freqs_long.shape}\")\n",
    "print(f\"   Theta: {theta_long:,.0f} (Slow Rotation)\")\n",
    "print(\"   Note: This larger theta keeps positions unique over long distances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming & Chunking\n",
    "\n",
    "### The TTT Advantage\n",
    "In standard Transformers, processing a 1 Million token document requires storing all 1M tokens in memory ($O(N)$ or $O(N^2)$). \n",
    "\n",
    "In **TTT**, we don't process the whole document at once. We process it in **Chunks**.\n",
    "\n",
    "1.  **Read Chunk 1**: Update Weights $\\rightarrow$ Discard tokens.\n",
    "2.  **Read Chunk 2**: Update Weights $\\rightarrow$ Discard tokens.\n",
    "3.  **Read Chunk 3**: ...\n",
    "\n",
    "The information persists in the **Updated Weights** ($W_t$), not in the token cache. This is why TTT has constant memory usage ($O(1)$) regardless of sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Streaming Simulation\n",
      "   Total Tokens: 1,000,000\n",
      "   Chunk Size: 4096\n",
      "   Total Updates Required: 244\n",
      "\n",
      "   TTT Flow:\n",
      "   [Start] -> [Chunk 1] -> [Update Weights] -> [Chunk 2] -> [Update Weights] ... -> [End]\n",
      "   Result: The final model has 'read' the whole book but never held it all in RAM at once.\n"
     ]
    }
   ],
   "source": [
    "# Simulation of a massive document stream\n",
    "total_tokens = 1_000_000 # A very long book (approx 1MB of text)\n",
    "chunk_size = 4096      # The size that typically fits in GPU VRAM\n",
    "\n",
    "num_chunks = total_tokens // chunk_size\n",
    "\n",
    "print(f\"\\n3. Streaming Simulation\")\n",
    "print(f\"   Total Tokens: {total_tokens:,}\")\n",
    "print(f\"   Chunk Size: {chunk_size}\")\n",
    "print(f\"   Total Updates Required: {num_chunks}\")\n",
    "\n",
    "print(\"\\n   TTT Flow:\")\n",
    "print(\"   [Start] -> [Chunk 1] -> [Update Weights] -> [Chunk 2] -> [Update Weights] ... -> [End]\")\n",
    "print(\"   Result: The final model has 'read' the whole book but never held it all in RAM at once.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
