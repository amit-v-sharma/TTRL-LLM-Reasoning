{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Introduction to Test-Time Training (TTT)\n",
    "\n",
    "## ðŸ“š Introduction\n",
    "\n",
    "Welcome to the first lesson in the **End-to-End Test-Time Training** tutorial series.\n",
    "\n",
    "### The Problem: Memory Bottlenecks\n",
    "Traditional Transformer models (like GPT-4, Llama 3) process text using **Attention**.\n",
    "- To \"remember\" what you said 5 minutes ago, they store a **Key-Value Cache (KV Cache)**.\n",
    "- **Short Context**: This is fine.\n",
    "- **Long Context (1M+ tokens)**: The KV Cache becomes massive. Storing millions of previous tokens in GPU RAM is slow and expensive (Terabytes of RAM needed).\n",
    "\n",
    "### The Solution: \"Context as Weights\"\n",
    "**Test-Time Training (TTT)** proposes a radical shift.\n",
    "Instead of storing the history in a static cache, we **train a neural network** on the history.\n",
    "\n",
    "- **Old Way (Attention)**: `Memory = [List of past tokens]`\n",
    "- **New Way (TTT)**: `Memory = [Weights of a Model]`\n",
    "\n",
    "The \"memory\" of the conversation is compressed into the updated weights of this internal network. As a new token arrives, we run a gradient descent step to update these weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the TTT Layer\n",
    "\n",
    "Here we will build a **Toy Model** from scratch to visualize how weights can be updated on-the-fly during a forward pass.\n",
    "\n",
    "### Why do we need a custom layer?\n",
    "Standard PyTorch layers (`nn.Linear`) are designed to have fixed weights during inference. We need a layer where the weights (`inner_weights`) are **mutable** and updated *per token*.\n",
    "\n",
    "### Key Components\n",
    "1.  **`inner_weights` (The \"Memory\")**: In a standard RNN, the hidden state $h_t$ is a vector. In TTT, the hidden state is a **Matrix** $W_t$ that we learn.\n",
    "2.  **`query`, `key`, `value` projections**: These are fixed weights learned during pre-training. They decide *what* to store in our memory matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleTTTLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, learning_rate):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # --- TRADITIONAL WEIGHTS (Fixed during inference) ---\n",
    "        # These are \"Meta-Parameters\". They don't change at test time.\n",
    "        # Their job is to process the input into signals that are useful for training the inner weights.\n",
    "        self.query_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(input_dim, hidden_dim) \n",
    "        self.value_proj = nn.Linear(input_dim, input_dim) \n",
    "        \n",
    "        # --- TTT WEIGHTS (Updated during inference) ---\n",
    "        # This matrix acts as the \"hidden state\". It starts at zero (Empty Memory).\n",
    "        # Shape: (Input, Hidden) mapping.\n",
    "        self.inner_weights = nn.Parameter(torch.zeros(input_dim, hidden_dim))\n",
    "        \n",
    "    def forward(self, x_sequence):\n",
    "        \"\"\"\n",
    "        Processing a sequence token-by-token using the TTT logic.\n",
    "        x_sequence: (Batch, SeqLen, Dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x_sequence.shape\n",
    "        outputs = []\n",
    "        \n",
    "        # --- [CRITICAL STEP]: Reset Memory ---\n",
    "        # Before reading a new document, we must wipe the short-term memory (inner weights).\n",
    "        # We utilize .clone() so we don't permanently overwrite the initialization for future runs.\n",
    "        current_W = self.inner_weights.clone() \n",
    "        \n",
    "        print(f\"\\nProcessing sequence of length {seq_len}...\")\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Extract current token\n",
    "            x_t = x_sequence[:, t, :] # (Batch, Dim)\n",
    "            \n",
    "            # =========================================================\n",
    "            # INNER LOOP: The \"Training\" Phase\n",
    "            # Use the current token to update our memory (current_W).\n",
    "            # =========================================================\n",
    "            \n",
    "            # 1. Create training signals from input\n",
    "            # k_t = What pattern are we looking for?\n",
    "            # v_t_target = What is the value associated with that pattern?\n",
    "            k_t = self.key_proj(x_t)\n",
    "            v_t_target = self.value_proj(x_t)\n",
    "            \n",
    "            # 2. Forward pass through our \"Memory Network\" (current_W)\n",
    "            # We check what our memory *currently* predicts given the key.\n",
    "            # Prediction = k_t * W\n",
    "            v_t_pred = torch.matmul(k_t, current_W.t()) \n",
    "            \n",
    "            # 3. Calculate Loss (Reconstruction Error)\n",
    "            # How well did our memory predict this token? \n",
    "            # If the error is high, it means this is 'new information' we need to store.\n",
    "            loss = torch.mean((v_t_pred - v_t_target) ** 2)\n",
    "            \n",
    "            # 4. Update the Weights (Gradient Descent step)\n",
    "            # We calculate how to change current_W to reduce the loss (i.e., remember this token).\n",
    "            grad = torch.autograd.grad(loss, current_W, create_graph=True)[0]\n",
    "            current_W = current_W - self.learning_rate * grad\n",
    "            \n",
    "            # =========================================================\n",
    "            # OUTER LOOP: The \"Inference\" Phase\n",
    "            # Use the UPDATED memory to generate/process the output.\n",
    "            # =========================================================\n",
    "            \n",
    "            q_t = self.query_proj(x_t)\n",
    "            \n",
    "            # Process query using the *updated* memory.\n",
    "            # This is equivalent to: Attention(Q, K, V)\n",
    "            output = torch.matmul(q_t, current_W.t()) \n",
    "            outputs.append(output)\n",
    "            \n",
    "            # Visualization logs\n",
    "            if t % 5 == 0:\n",
    "                print(f\"  Token {t}: Adaptation Loss = {loss.item():.6f}\")\n",
    "\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running the Simulation\n",
    "\n",
    "Now we run the model on a random sequence of inputs.\n",
    "\n",
    "### What to look for:\n",
    "Watch the `Adaptation Loss` printed in the loop.\n",
    "- **High Loss**: The model is encountering new data.\n",
    "- **Decreasing Loss**: The model is successfully \"learning\" the patterns in the sequence.\n",
    "\n",
    "**Observation**: The variable `current_W` (our hidden state) is evolving *per token*. This variable is effectively a compressed representation of the entire history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TTT Layer...\n",
      "Standard Input Shape: torch.Size([1, 20, 8]) (Batch, Seq, Dim)\n",
      "\n",
      "Processing sequence of length 20...\n",
      "  Token 0: Adaptation Loss = 0.566423\n",
      "  Token 5: Adaptation Loss = 0.221849\n",
      "  Token 10: Adaptation Loss = 0.126281\n",
      "  Token 15: Adaptation Loss = 0.523686\n",
      "Final Output Shape: torch.Size([1, 20, 8])\n",
      "\n",
      "âœ… Done! You have successfully run a Test-Time Training loop.\n"
     ]
    }
   ],
   "source": [
    "input_dim = 8\n",
    "hidden_dim = 16\n",
    "lr = 0.1 # High learning rate to make the adaptation obvious\n",
    "seq_len = 20\n",
    "\n",
    "print(\"Initializing TTT Layer...\")\n",
    "model = SimpleTTTLayer(input_dim, hidden_dim, lr)\n",
    "\n",
    "# Create random input sequence\n",
    "# Batch size = 1, Sequence Length = 20, Dimensions = 8\n",
    "x = torch.randn(1, seq_len, input_dim)\n",
    "\n",
    "print(f\"Standard Input Shape: {x.shape} (Batch, Seq, Dim)\")\n",
    "\n",
    "# Run the Forward Pass (which includes the Inner Training Loop)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Final Output Shape: {output.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Done! You have successfully run a Test-Time Training loop.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
