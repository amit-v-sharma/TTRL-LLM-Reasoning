{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: The Dual Learning Loop (Expert Iteration)\n",
    "\n",
    "## ‚ôæÔ∏è The Self-Improving Machine\n",
    "This is the architecture used by **DeepSeek-R1** and **OpenAI o1** to achieve superhuman performance. It connects the two previous concepts into a loop.\n",
    "\n",
    "**The Concept: Expert Iteration (STaR)**\n",
    "1.  **System 2 Search (Inference)**: The model struggles and tries multiple paths (Best-of-N). It finds a \"Gold Solution\" that is much better than its average output.\n",
    "2.  **Filter**: We discard the failed attempts.\n",
    "3.  **System 1 Update (Training)**: We train the model on its own \"Gold Solution\".\n",
    "\n",
    "**The Result**: The model \"memorizes\" the complex reasoning path. Next time, it outputs the smart answer instantly (System 1) without needing expensive search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Setup Device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Step 1: Simulate the \"Search\" Phase\n",
    "Imagine we ask Qwen a tricky riddle.\n",
    "Normally, it fails. But if we let it generate 5 different guesses, **one** of them happens to be correct.\n",
    "\n",
    "In a real pipeline, we would run `ollama.generate()` 100 times here. For the tutorial, we simulate finding that needle in the haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM = \"There are 5 birds on a wire. Hunters shoot 2. How many are left?\"\n",
    "\n",
    "# SIMULATED SEARCH RESULTS\n",
    "candidates = [\n",
    "    \"There are 3 birds left because 5 - 2 = 3.\", # Fail (Literal)\n",
    "    \"3 birds remain.\", # Fail\n",
    "    \"None. The remaining birds flew away after the noise.\", # SUCCESS! (Lateral Thinking)\n",
    "    \"There are 3.\", # Fail\n",
    "    \"Probably 3.\"\n",
    "]\n",
    "\n",
    "# FILTER: We select ONLY the correct reasoning\n",
    "correct_solution = [c for c in candidates if \"None\" in c][0]\n",
    "print(f\"Found Gold Solution via Search: {correct_solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Step 2: Format for SFT (Supervised Fine-Tuning)\n",
    "Now we assume this successful answer is the \"Ground Truth\". We want to update the model to output this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Format for SFT\n",
    "train_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": PROBLEM},\n",
    "            {\"role\": \"assistant\", \"content\": correct_solution}\n",
    "        ]\n",
    "    }\n",
    "] * 10 # Repeat to force learning in this tiny demo\n",
    "\n",
    "dataset = Dataset.from_list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Step 3: The Update (Fine-Tuning)\n",
    "We use `SFTTrainer`. This uses standard Causal Language Modeling (Next Token Prediction).\n",
    "We are maximizing the probability of the tokens: \"None... flew... away...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# LoRA Config (Train only 1% of params)\n",
    "peft_config = LoraConfig(\n",
    "    r=8, lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./self_improved_model\",\n",
    "    max_steps=10, # Tiny training run\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    use_mps_device=True,\n",
    "    fp16=False, \n",
    "    dataset_text_field=\"text\", # Ignored for chat format\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting Self-Improvement Loop...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Final Conclusion\n",
    "You have just walked through the entire pipeline of modern **Post-Training**:\n",
    "\n",
    "1.  **Inference**: Sampling models can solve harder problems than greedy models.\n",
    "2.  **Verification**: We need Verifiers (Reward Models) to filter those samples.\n",
    "3.  **Iteration**: We feed the best samples back into training to make the base model smarter.\n",
    "\n",
    "This loop is how we get to AGI. üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
