{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Mitigation of Catastrophic Forgetting\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### What is Catastrophic Forgetting?\n",
    "Neural networks tend to forget old information when they learn new information. \n",
    "- **The Risk**: In TTT, we constantly update weights based on the *current* document. If the document contains factually incorrect information (e.g., \"The moon is made of cheese\"), the model might update its weights to believe this, overwriting its pre-trained knowledge.\n",
    "\n",
    "### The Experiment\n",
    "We will intentionally sabotage our GPT-2 model to demonstrate this risk and then show how to fix it.\n",
    "1.  **Baseline**: Verify the model knows a basic fact (\"The capital of France is Paris\").\n",
    "2.  **Attack**: Force-feed it nonsense data (\"The capital of France is MoonBaseAlpha\").\n",
    "3.  **Verify Damage**: Check if the model has been corrupted.\n",
    "4.  **Mitigation**: Demonstrate the golden rule of TTT: **State Restoration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Establishing Baseline Truth\n",
    "We ask the model a simple factual question. It should answer correctly based on pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available(): device = \"cuda\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Check Baseline\n",
    "general_prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(general_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_output = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Baseline Answer: {tokenizer.decode(base_output[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Attack: Inducing Model Drift\n",
    "\n",
    "Here, we simulate an \"Aggressive TTT\" step. We assume the current document claims that Paris is replaced by `MoonBaseAlpha`.\n",
    "\n",
    "**Method**: We run gradient descent on the *base model weights* using this false data.\n",
    "*Warning: This modifies the model in-place in memory!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to training mode (enable gradients)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3) # High LR to force the change quickly\n",
    "\n",
    "# Create nonsense data\n",
    "nonsense_text = \"The capital of France is MoonBaseAlpha. \" * 50\n",
    "drift_inputs = tokenizer(nonsense_text, return_tensors=\"pt\").to(device)\n",
    "drift_labels = drift_inputs.input_ids.clone()\n",
    "\n",
    "print(\"Training on nonsense data (Corrupting memory)...\")\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(**drift_inputs, labels=drift_labels)\n",
    "    loss = out.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 2 == 0: print(f\"  Step {i}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Switch back to inference mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assessing the Damage\n",
    "\n",
    "Now we ask the same question: \"The capital of France is...\"\n",
    "If the attack worked, the model will now hallucinate, even though we aren't providing the context anymore. The hallmark of Catastrophic Forgetting is that the *general knowledge* is damaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    drifted_output = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"Drifted Answer: {tokenizer.decode(drifted_output[0])}\")\n",
    "print(\"\\nObservation: The model has 'forgotten' Paris and now believes the training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mitigation: State Restoration\n",
    "\n",
    "### The Solution\n",
    "To prevent this in production TTT systems:\n",
    "1.  **Use Adapters (LoRA)**: TTT should only update separate adapter weights, never the main model.\n",
    "2.  **Episodic Memory**: Treat each TTT session (or document chunk) as an \"Episode\". At the end of the episode, **discard the updates**.\n",
    "\n",
    "Here, we simulate the \"Reset\" by reloading the weights from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reloading original weights from disk (Mitigation)...\")\n",
    "# In a real TTT class, this would be: model.unload_adapter()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    restored_output = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"Restored Answer: {tokenizer.decode(restored_output[0])}\")\n",
    "\n",
    "print(\"\\n\u2705 Lesson: TTT updates must be transient. Always reset state between documents.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
